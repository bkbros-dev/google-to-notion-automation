# import os
# import re
# import tempfile
# import base64
# import requests
# from datetime import datetime, timedelta
# import boto3
# import chromedriver_autoinstaller
# from selenium.webdriver.chrome.service import Service
# from urllib.parse import urlparse, unquote
# from PIL import Image
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC

# from google.oauth2 import service_account
# from googleapiclient.discovery import build
# from notion_client import Client as NotionClient

# # â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (GitHub Actionsìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CHROME_BINARY = os.getenv("CHROME_BINARY", "/usr/bin/google-chrome")
# SMORE_USER_DATA = os.getenv("SMORE_USER_DATA")
# SMORE_PROFILE = os.getenv("SMORE_PROFILE")

# # Google ì¸ì¦ íŒŒì¼ ì²˜ë¦¬
# GOOGLE_CRED = os.getenv(
#     "GOOGLE_APPLICATION_CREDENTIALS", "/tmp/google_credentials.json"
# )

# SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
# SHEET_RANGE = os.getenv("SHEET_RANGE")
# AWS_REGION = os.getenv("AWS_DEFAULT_REGION")
# S3_BUCKET = os.getenv("S3_BUCKET_NAME")
# NOTION_TOKEN = os.getenv("NOTION_TOKEN")
# NOTION_DB_ID = os.getenv("NOTION_DATABASE_ID")
# TEST_OFFSET = int(os.getenv("TEST_OFFSET", "0"))
# TEST_LIMIT = int(os.getenv("TEST_LIMIT", "0"))

# # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬
# DOWNLOAD_DIR = os.getenv("SMORE_DOWNLOAD_DIR", tempfile.gettempdir())
# os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# # â”€â”€â”€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
# ga_creds = service_account.Credentials.from_service_account_file(
#     GOOGLE_CRED, scopes=SCOPES
# )
# sheets = build("sheets", "v4", credentials=ga_creds)

# s3 = boto3.client("s3", region_name=AWS_REGION)
# notion = NotionClient(auth=NOTION_TOKEN)

# SHEET_NAME = SHEET_RANGE.split("!")[0]

# # â”€â”€â”€ í—¤ë” ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HEADER_TO_PROP = {
#     "ìˆœë²ˆ": "ìˆœë²ˆ",
#     "ì¶œì‚°ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì„¸ìš” ë“±ë³¸, ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ, ì¶œìƒì¦ëª…ì„œ, ì‚°ëª¨ìˆ˜ì²© ì¤‘ íƒ1": "ì¦ëª…ì„œ ì´ë¯¸ì§€",
#     "íƒ€ê°€ëª°ì—ì„œ ì‚¬ìš© ì¤‘ì´ì‹  ì•„ì´ë””ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’› ê°„í¸ê°€ì… í•˜ì‹  ê²½ìš° ì•± ì•„ì´ë””ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.(ex.1234567@N)": "íƒ€ê°€ëª° ì•„ì´ë””",
#     "ì‹ ì²­ì ë³¸ì¸ì˜ ì„±í•¨ì„ ì ì–´ì£¼ì„¸ìš” ğŸ˜ƒ â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì´ë¦„",
#     "ğŸ“± ì—°ë½ ê°€ëŠ¥í•œ ë²ˆí˜¸ë¥¼ ì ì–´ì£¼ì„¸ìš”ğŸ’› (ì˜ˆ: 010-1234-5678)": "ì—°ë½ì²˜",
#     "ë³´ë“¬ë°•ìŠ¤ë¥¼ ë°›ìœ¼ì‹¤ ìƒì„¸ ì£¼ì†Œë¥¼ ì ì–´ ì£¼ì„¸ìš”.ğŸ’› â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì£¼ì†Œ",
#     "ğŸ“… ì•„ê¸° ì¶œìƒ ì˜ˆì •ì¼ì´ë‚˜ ì¶œìƒì¼ì„ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’›": "ì¶œìƒì¼",
#     "ë‹´ë‹¹ì": "ë‹´ë‹¹ì",
#     "ì /ë¶€": "ì /ë¶€",
# }


# def normalize_header(h: str) -> str:
#     return re.sub(r"\s+", " ", h.replace("\n", " ").strip())


# NORM_HEADER_TO_PROP = {normalize_header(k): v for k, v in HEADER_TO_PROP.items()}


# # â”€â”€â”€ Excel serial date ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def excel_serial_to_datetime(serial) -> datetime:
#     try:
#         days = float(serial)
#     except:
#         return None
#     epoch = datetime(1899, 12, 30) if days >= 61 else datetime(1899, 12, 31)
#     return epoch + timedelta(days=days)


# # â”€â”€â”€ íŒŒì¼ëª… ì•ˆì „ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def safe_key_name(row_idx: int, filename: str) -> str:
#     base, ext = os.path.splitext(filename)
#     b64 = base64.urlsafe_b64encode(base.encode("utf-8")).decode("ascii")
#     return f"row{row_idx}_{b64}{ext}"


# # â”€â”€â”€ S3 ì—…ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # def upload_to_s3(local: str, key: str) -> str:
# # import mimetypes


# # ctype, _ = mimetypes.guess_type(local)
# # s3.upload_file(
# #     Filename=local,
# #     Bucket=S3_BUCKET,
# #     Key=key,
# #     ExtraArgs={
# #         "ACL": "public-read",
# #         "ContentType": ctype or "application/octet-stream",
# #     },
# # )
# # return f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"
# # ì´ë¯¸ì§€ì¸ ê²½ìš° ì••ì¶•
# def upload_to_s3(local, key):
#     # ì´ë¯¸ì§€ ì••ì¶• ë° ë¦¬ì‚¬ì´ì¦ˆ ì‹œë„
#     try:
#         from PIL import Image

#         img = Image.open(local)
#         # ìµœëŒ€ ê°€ë¡œ/ì„¸ë¡œ 1024pxë¡œ ë¦¬ì‚¬ì´ì¦ˆ
#         resample = getattr(Image, "Resampling", Image).LANCZOS
#         img.thumbnail((1024, 1024), resample)
#         ext = os.path.splitext(local)[1].lower()
#         quality = 75  # ìˆ˜ì •: í’ˆì§ˆ ë‚®ì¶° ìš©ëŸ‰ ì¤„ì´ê¸°
#         if ext in (".jpg", ".jpeg"):
#             img.save(local, format="JPEG", optimize=True, quality=quality)
#         else:
#             # PNG ë“±ì€ JPEGë¡œ ë³€í™˜
#             rgb = img.convert("RGB")
#             jpeg_path = os.path.splitext(local)[0] + ".jpg"
#             rgb.save(jpeg_path, format="JPEG", optimize=True, quality=quality)
#             local = jpeg_path
#     except ImportError:
#         # PIL ë¯¸ì„¤ì¹˜ ì‹œ ì••ì¶• ìƒëµ
#         pass
#     except Exception as e:
#         print(f"[ì••ì¶• ì˜¤ë¥˜] {local}: {e}")
#     # S3 ì—…ë¡œë“œ
#     import mimetypes

#     ctype, _ = mimetypes.guess_type(local)
#     s3.upload_file(
#         Filename=local,
#         Bucket=S3_BUCKET,
#         Key=key,
#         ExtraArgs={
#             "ACL": "public-read",
#             "ContentType": ctype or "application/octet-stream",
#         },
#     )
#     return f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"


# def get_smore_cookies():
#     driver_path = chromedriver_autoinstaller.install()
#     opts = Options()
#     opts.binary_location = CHROME_BINARY

#     # GitHub Actions í™˜ê²½ìš© ì˜µì…˜ ì¶”ê°€
#     opts.add_argument("--headless")
#     opts.add_argument("--no-sandbox")
#     opts.add_argument("--disable-dev-shm-usage")
#     opts.add_argument("--disable-gpu")
#     opts.add_argument("--remote-debugging-port=9222")

#     # ê¸°ì¡´ ì˜µì…˜
#     if SMORE_USER_DATA:
#         opts.add_argument(f"--user-data-dir={SMORE_USER_DATA}")
#     if SMORE_PROFILE:
#         opts.add_argument(f"--profile-directory={SMORE_PROFILE}")

#     driver = webdriver.Chrome(service=Service(driver_path), options=opts)
#     try:
#         driver.get("https://smore.im")
#         return {c["name"]: c["value"] for c in driver.get_cookies()}
#     finally:
#         driver.quit()


# def download_smore_image(page_url, cookies, row_idx):
#     driver_path = chromedriver_autoinstaller.install()
#     opts = Options()
#     opts.binary_location = CHROME_BINARY
#     opts.add_argument(f"--user-data-dir={SMORE_USER_DATA}")
#     opts.add_argument(f"--profile-directory={SMORE_PROFILE}")
#     driver = webdriver.Chrome(service=Service(driver_path), options=opts)
#     try:
#         driver.get(page_url)
#         link = WebDriverWait(driver, 15).until(
#             EC.element_to_be_clickable((By.ID, "download"))
#         )
#         file_url = link.get_attribute("href")
#         sess = requests.Session()
#         sess.cookies.update(cookies)
#         resp = sess.get(file_url, stream=True, timeout=30)
#     finally:
#         driver.quit()

#     resp.raise_for_status()
#     if "text/html" in resp.headers.get("Content-Type", ""):
#         raise RuntimeError("HTML ë°˜í™˜ë¨(ë¡œê·¸ì¸ í•„ìš”?): " + page_url)

#     cd = resp.headers.get("content-disposition", "")
#     m = re.search(r"filename\*=UTF-8''([^;]+)", cd) or re.search(
#         r"filename=?\"?([^\";]+)\"?", cd
#     )
#     orig = unquote(m.group(1)) if m else os.path.basename(urlparse(file_url).path)
#     filename = safe_key_name(row_idx, orig)
#     local = os.path.join(DOWNLOAD_DIR, filename)
#     with open(local, "wb") as f:
#         for chunk in resp.iter_content(1024):
#             f.write(chunk)
#     return local


# # â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def sheet_to_notion_s3():
#     cookies = get_smore_cookies()
#     res = (
#         sheets.spreadsheets()
#         .values()
#         .get(
#             spreadsheetId=SPREADSHEET_ID, range=SHEET_RANGE, valueRenderOption="FORMULA"
#         )
#         .execute()
#     )
#     rows = res.get("values", [])
#     if len(rows) < 2:
#         print("ë°ì´í„° ì—†ìŒ")
#         return

#     headers = [normalize_header(h) for h in rows[0]]
#     done_idx = headers.index("ì´ê´€ì™„ë£Œ") if "ì´ê´€ì™„ë£Œ" in headers else None
#     data_rows = rows[1:]
#     recs = data_rows[TEST_OFFSET : TEST_OFFSET + TEST_LIMIT]
#     db_props = notion.databases.retrieve(database_id=NOTION_DB_ID)["properties"]

#     for i, row in enumerate(recs, start=TEST_OFFSET + 2):
#         # â”€â”€â”€ ì—¬ê¸°ì„œ Sì—´ 'ì™„ë£Œ' ì²´í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#         if done_idx is not None and len(row) > done_idx:
#             cell_val = row[done_idx].strip()
#             # 'ì™„ë£Œ'ë¡œ ì‹œì‘(ì˜ˆ: "ì™„ë£Œ", "ì™„ë£Œ<")í•˜ë©´ ê±´ë„ˆëœë‹ˆë‹¤
#             if cell_val.startswith("ì™„ë£Œ"):
#                 print(f"[Row {i}] ì´ë¯¸ ì™„ë£Œëœ í–‰, ê±´ë„ˆëœë‹ˆë‹¤")
#                 continue
#         data = {headers[j]: row[j] if j < len(row) else "" for j in range(len(headers))}
#         props, image_urls = {}, []

#         for hdr, val in data.items():
#             if not val:
#                 continue
#             prop = NORM_HEADER_TO_PROP.get(normalize_header(hdr))
#             if not prop or prop not in db_props:
#                 continue
#             ptype = db_props[prop]["type"]
#             if ptype == "files":
#                 # m = re.match(r'=HYPERLINK\("([^"\)]+)"', val)
#                 # url = m.group(1) if m else val
#                 m = re.match(r'=HYPERLINK\("([^"]+)"(?:\s*,\s*"([^"]+)")?\)', val)
#                 url = m.group(1)  # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL
#                 disp = m.group(2) if m and m.group(2) else None  # ì˜µì…˜ í‘œì‹œëª…
#                 try:
#                     local = download_smore_image(url, cookies, i)
#                     key = safe_key_name(i, os.path.basename(local))
#                     s3_url = upload_to_s3(local, key)
#                     original_name = disp or os.path.basename(local)
#                     if len(original_name) > 100:
#                         display_name = original_name[:97] + "..."
#                     else:
#                         display_name = original_name
#                     props[prop] = {
#                         "files": [
#                             {
#                                 # "name": os.path.basename(local),
#                                 "name": display_name,
#                                 "external": {"url": s3_url},
#                             }
#                         ]
#                     }
#                     image_urls.append(s3_url)
#                 except Exception as e:
#                     print(f"[Row {i}] íŒŒì¼ ì˜¤ë¥˜: {e}")
#                 continue
#             if ptype == "date":
#                 m2 = re.match(
#                     r"^=DATE\(\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*\)", str(val)
#                 )
#                 if m2:
#                     y, mo, da = map(int, m2.groups())
#                     props[prop] = {"date": {"start": datetime(y, mo, da).isoformat()}}
#                 else:
#                     dt = excel_serial_to_datetime(val)
#                     if dt:
#                         props[prop] = {"date": {"start": dt.isoformat()}}
#                 continue
#             if ptype == "url":
#                 props[prop] = {"url": val.strip()}
#                 continue
#             if ptype == "title":
#                 props[prop] = {"title": [{"text": {"content": str(val)}}]}
#                 continue
#             if ptype == "rich_text":
#                 props[prop] = {"rich_text": [{"text": {"content": str(val)}}]}
#                 continue

#         if props:
#             # print(f"[Row {i}] props=", props)  # debug
#             page = notion.pages.create(
#                 parent={"database_id": NOTION_DB_ID}, properties=props
#             )
#             print(f"[Row {i}] Notion ë“±ë¡ ì™„ë£Œ", list(props.keys()))

#             # ìˆ˜ì •: Google Sheets Sì—´ì— 'ì™„ë£Œ' í‘œì‹œ
#             cell = f"{SHEET_NAME}!S{i}"
#             sheets.spreadsheets().values().update(
#                 spreadsheetId=SPREADSHEET_ID,
#                 range=cell,
#                 valueInputOption="USER_ENTERED",
#                 body={"values": [["ì™„ë£Œ"]]},
#             ).execute()
#             print(f"[Row {i}] ì‹œíŠ¸ì— â€˜ì™„ë£Œâ€™ í‘œê¸° ì™„ë£Œ â†’ {cell}")

#             # â”€â”€â”€ ë¸”ë¡ ì‚½ì… (ì˜¤ë¥˜ ë‚˜ë©´ ê±´ë„ˆë›°ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#             for url in image_urls:
#                 ext = os.path.splitext(urlparse(url).path)[1].lower()
#                 if ext == ".pdf":
#                     block = {
#                         "object": "block",
#                         "type": "file",
#                         "file": {"type": "external", "external": {"url": url}},
#                     }
#                 else:
#                     block = {
#                         "object": "block",
#                         "type": "image",
#                         "image": {"type": "external", "external": {"url": url}},
#                     }

#                 try:
#                     notion.blocks.children.append(block_id=page["id"], children=[block])
#                     print(f"[Row {i}] ë¸”ë¡ ì‚½ì… ì™„ë£Œ â†’ {url}")
#                 except Exception as e:
#                     print(f"[Row {i}] ë¸”ë¡ ì‚½ì… ì˜¤ë¥˜, ê±´ë„ˆëœë‹ˆë‹¤: {url} â–¶ {e}")

#         else:
#             print(f"[Row {i}] ë§¤í•‘ëœ ì†ì„± ì—†ìŒ")


# if __name__ == "__main__":
#     sheet_to_notion_s3()

import os
import re
import tempfile
import base64
import requests
import json
from datetime import datetime, timedelta
import boto3
from dotenv import load_dotenv
from urllib.parse import urlparse, unquote
from PIL import Image

from google.oauth2 import service_account
from googleapiclient.discovery import build
from notion_client import Client as NotionClient

# â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()


def get_env_or_fail(key, required=True):
    """í™˜ê²½ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ë˜, í•„ìˆ˜ê°’ì´ ì—†ìœ¼ë©´ ì—ëŸ¬"""
    value = os.getenv(key)
    if required and not value:
        raise ValueError(f"í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ {key}ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return value


# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
GOOGLE_CRED = get_env_or_fail("GOOGLE_APPLICATION_CREDENTIALS", False)
GOOGLE_CRED_JSON = get_env_or_fail("GOOGLE_CREDENTIALS_JSON", False)
SPREADSHEET_ID = get_env_or_fail("SPREADSHEET_ID")
SHEET_RANGE = get_env_or_fail("SHEET_RANGE")
AWS_REGION = os.getenv("AWS_DEFAULT_REGION", "ap-northeast-2")
S3_BUCKET = get_env_or_fail("S3_BUCKET_NAME")
NOTION_TOKEN = get_env_or_fail("NOTION_TOKEN")
NOTION_DB_ID = get_env_or_fail("NOTION_DATABASE_ID")
TEST_OFFSET = int(os.getenv("TEST_OFFSET", "0"))
TEST_LIMIT = int(os.getenv("TEST_LIMIT", "0"))

# Smore ì¿ í‚¤ (JSON í˜•íƒœ)
SMORE_COOKIES = os.getenv("SMORE_COOKIES", "{}")

# ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬
DOWNLOAD_DIR = os.getenv("DOWNLOAD_DIR", tempfile.gettempdir())
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

print("ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸:")
print(f"  - Google Creds: {'âœ…' if GOOGLE_CRED or GOOGLE_CRED_JSON else 'âŒ'}")
print(f"  - AWS Region: {AWS_REGION}")
print(f"  - S3 Bucket: {S3_BUCKET}")
print(f"  - Download Dir: {DOWNLOAD_DIR}")

# â”€â”€â”€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# Google Credentials ì²˜ë¦¬
if GOOGLE_CRED and os.path.exists(GOOGLE_CRED):
    ga_creds = service_account.Credentials.from_service_account_file(
        GOOGLE_CRED, scopes=SCOPES
    )
    print("âœ… Google ì¸ì¦: íŒŒì¼ ê¸°ë°˜")
elif GOOGLE_CRED_JSON:
    try:
        cred_data = json.loads(GOOGLE_CRED_JSON)
        ga_creds = service_account.Credentials.from_service_account_info(
            cred_data, scopes=SCOPES
        )
        print("âœ… Google ì¸ì¦: JSON ê¸°ë°˜")
    except json.JSONDecodeError:
        raise ValueError("GOOGLE_CREDENTIALS_JSONì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤.")
else:
    raise ValueError("Google Credentialsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
sheets = build("sheets", "v4", credentials=ga_creds)
s3 = boto3.client("s3", region_name=AWS_REGION)
notion = NotionClient(auth=NOTION_TOKEN)

# AWS ì—°ê²° í…ŒìŠ¤íŠ¸
try:
    s3.head_bucket(Bucket=S3_BUCKET)
    print("âœ… S3 ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"âŒ S3 ì—°ê²° ì‹¤íŒ¨: {e}")

SHEET_NAME = SHEET_RANGE.split("!")[0]

# â”€â”€â”€ í—¤ë” ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADER_TO_PROP = {
    "ìˆœë²ˆ": "ìˆœë²ˆ",
    "ì¶œì‚°ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì„¸ìš” ë“±ë³¸, ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ, ì¶œìƒì¦ëª…ì„œ, ì‚°ëª¨ìˆ˜ì²© ì¤‘ íƒ1": "ì¦ëª…ì„œ ì´ë¯¸ì§€",
    "íƒ€ê°€ëª°ì—ì„œ ì‚¬ìš© ì¤‘ì´ì‹  ì•„ì´ë””ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’› ê°„í¸ê°€ì… í•˜ì‹  ê²½ìš° ì•± ì•„ì´ë””ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.(ex.1234567@N)": "íƒ€ê°€ëª° ì•„ì´ë””",
    "ì‹ ì²­ì ë³¸ì¸ì˜ ì„±í•¨ì„ ì ì–´ì£¼ì„¸ìš” ğŸ˜ƒ â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì´ë¦„",
    "ğŸ“± ì—°ë½ ê°€ëŠ¥í•œ ë²ˆí˜¸ë¥¼ ì ì–´ì£¼ì„¸ìš”ğŸ’› (ì˜ˆ: 010-1234-5678)": "ì—°ë½ì²˜",
    "ë³´ë“¬ë°•ìŠ¤ë¥¼ ë°›ìœ¼ì‹¤ ìƒì„¸ ì£¼ì†Œë¥¼ ì ì–´ ì£¼ì„¸ìš”.ğŸ’› â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì£¼ì†Œ",
    "ğŸ“… ì•„ê¸° ì¶œìƒ ì˜ˆì •ì¼ì´ë‚˜ ì¶œìƒì¼ì„ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’›": "ì¶œìƒì¼",
    "ë‹´ë‹¹ì": "ë‹´ë‹¹ì",
    "ì /ë¶€": "ì /ë¶€",
}


def normalize_header(h: str) -> str:
    return re.sub(r"\s+", " ", h.replace("\n", " ").strip())


NORM_HEADER_TO_PROP = {normalize_header(k): v for k, v in HEADER_TO_PROP.items()}

# â”€â”€â”€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def excel_serial_to_datetime(serial) -> datetime:
    try:
        days = float(serial)
    except:
        return None
    epoch = datetime(1899, 12, 30) if days >= 61 else datetime(1899, 12, 31)
    return epoch + timedelta(days=days)


def safe_key_name(row_idx: int, filename: str) -> str:
    base, ext = os.path.splitext(filename)
    b64 = base64.urlsafe_b64encode(base.encode("utf-8")).decode("ascii")
    return f"row{row_idx}_{b64}{ext}"


# â”€â”€â”€ S3 ì—…ë¡œë“œ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def upload_to_s3(local_path, key):
    """ì´ë¯¸ì§€ ì••ì¶• ë° S3 ì—…ë¡œë“œ"""
    print(f"ğŸ“¤ S3 ì—…ë¡œë“œ ì‹œì‘: {key}")

    if not os.path.exists(local_path):
        raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {local_path}")

    compressed_path = local_path

    # ì´ë¯¸ì§€ ì••ì¶•
    try:
        with Image.open(local_path) as img:
            if img.mode in ("RGBA", "LA", "P"):
                img = img.convert("RGB")

            resample = getattr(Image, "Resampling", Image).LANCZOS
            img.thumbnail((1024, 1024), resample)

            jpeg_path = os.path.splitext(local_path)[0] + ".jpg"
            img.save(jpeg_path, format="JPEG", optimize=True, quality=75)
            compressed_path = jpeg_path
            key = os.path.splitext(key)[0] + ".jpg"

            if jpeg_path != local_path:
                try:
                    os.remove(local_path)
                except:
                    pass

        print(f"ğŸ—œï¸ ì´ë¯¸ì§€ ì••ì¶• ì™„ë£Œ: {compressed_path}")

    except Exception as e:
        print(f"âš ï¸ ì••ì¶• ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
        compressed_path = local_path

    # S3 ì—…ë¡œë“œ
    try:
        file_size = os.path.getsize(compressed_path)
        print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:,} bytes")

        import mimetypes

        content_type, _ = mimetypes.guess_type(compressed_path)

        with open(compressed_path, "rb") as f:
            s3.upload_fileobj(
                f,
                S3_BUCKET,
                key,
                ExtraArgs={
                    "ACL": "public-read",
                    "ContentType": content_type or "image/jpeg",
                    "CacheControl": "max-age=31536000",
                },
            )

        s3_url = f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"
        print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_url}")

        try:
            os.remove(compressed_path)
        except:
            pass

        return s3_url

    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€ Smore ì¿ í‚¤ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_smore_cookies():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¿ í‚¤ ë¡œë“œ"""
    try:
        if SMORE_COOKIES and SMORE_COOKIES != "{}":
            cookies = json.loads(SMORE_COOKIES)
            print(f"âœ… ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ: {len(cookies)}ê°œ")
            return cookies
        else:
            print("âš ï¸ ì¿ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. ìˆ˜ë™ ì„¤ì • í•„ìš”.")
            return {}
    except json.JSONDecodeError:
        print("âŒ ì¿ í‚¤ JSON íŒŒì‹± ì‹¤íŒ¨")
        return {}


# â”€â”€â”€ Smore íŒŒì¼ ë‹¤ìš´ë¡œë“œ (BeautifulSoup ì™„ì „ ì œê±°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_smore_image_direct(page_url, cookies, row_idx):
    """BeautifulSoup ì—†ì´ Smore APIì—ì„œ ì§ì ‘ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    print(f"â¬‡ï¸ Smore API ë‹¤ìš´ë¡œë“œ ì‹œì‘: Row {row_idx}")

    try:
        session = requests.Session()

        # ì¿ í‚¤ ì„¤ì •
        if cookies:
            session.cookies.update(cookies)

        # í—¤ë” ì„¤ì •
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "image/webp,image/apng,image/*,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "Sec-Fetch-Dest": "image",
            "Sec-Fetch-Mode": "no-cors",
            "Sec-Fetch-Site": "same-origin",
            "Referer": "https://smore.im/",
        }

        print(f"ğŸ”— API ì§ì ‘ í˜¸ì¶œ: {page_url}")

        # ë¨¼ì € HEAD ìš”ì²­ìœ¼ë¡œ í™•ì¸
        try:
            head_response = session.head(page_url, headers=headers, timeout=10)
            print(f"ğŸ“Š HEAD ì‘ë‹µ: {head_response.status_code}")
            print(
                f"ğŸ“‹ Content-Type: {head_response.headers.get('Content-Type', 'Unknown')}"
            )
            print(
                f"ğŸ“ Content-Length: {head_response.headers.get('Content-Length', 'Unknown')}"
            )
        except:
            print("âš ï¸ HEAD ìš”ì²­ ì‹¤íŒ¨, GETìœ¼ë¡œ ì§„í–‰")

        # ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        response = session.get(page_url, headers=headers, stream=True, timeout=60)
        print(f"ğŸ“Š GET ì‘ë‹µ: {response.status_code}")

        if response.status_code != 200:
            print(f"âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
            return create_error_image(row_idx, f"HTTP {response.status_code}")

        # Content-Type í™•ì¸
        content_type = response.headers.get("Content-Type", "").lower()
        print(f"ğŸ“‹ ì‹¤ì œ Content-Type: {content_type}")

        # HTML ì‘ë‹µì´ë©´ ì¸ì¦ ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜
        if "text/html" in content_type:
            print("âŒ HTML ì‘ë‹µ ê°ì§€ - ì¸ì¦ ì‹¤íŒ¨ ë˜ëŠ” ì ‘ê·¼ ê±°ë¶€")
            # ì‘ë‹µ ë‚´ìš© ì¼ë¶€ í™•ì¸
            try:
                content_preview = response.text[:300]
                print(f"ğŸ“„ HTML ë¯¸ë¦¬ë³´ê¸°: {content_preview}")

                # íŠ¹ì • ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸
                if any(
                    keyword in content_preview.lower()
                    for keyword in [
                        "login",
                        "unauthorized",
                        "forbidden",
                        "access denied",
                    ]
                ):
                    return create_error_image(row_idx, "ì¸ì¦ í•„ìš”")
                else:
                    return create_error_image(row_idx, "HTML ì‘ë‹µ")
            except:
                return create_error_image(row_idx, "ì¸ì¦ ì˜¤ë¥˜")

        # JSON ì‘ë‹µì´ë©´ API ì˜¤ë¥˜
        if "application/json" in content_type:
            try:
                error_data = response.json()
                print(f"ğŸ“„ JSON ì˜¤ë¥˜: {error_data}")
                error_msg = error_data.get("message", "API ì˜¤ë¥˜")
                return create_error_image(row_idx, error_msg)
            except:
                return create_error_image(row_idx, "JSON íŒŒì‹± ì˜¤ë¥˜")

        # íŒŒì¼ëª… ì¶”ì¶œ
        filename = f"smore_file_{row_idx}.jpg"
        content_disposition = response.headers.get("content-disposition", "")
        if content_disposition:
            # Content-Disposition íŒŒì‹±
            patterns = [
                r"filename\*=UTF-8\'\'([^;]+)",
                r'filename="([^"]+)"',
                r"filename=([^;,]+)",
            ]
            for pattern in patterns:
                match = re.search(pattern, content_disposition, re.IGNORECASE)
                if match:
                    filename = unquote(match.group(1).strip('"'))
                    print(f"ğŸ“ ì¶”ì¶œëœ íŒŒì¼ëª…: {filename}")
                    break

        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        safe_filename = safe_key_name(row_idx, filename)
        local_path = os.path.join(DOWNLOAD_DIR, safe_filename)

        # íŒŒì¼ ì €ì¥
        total_size = 0
        print("ğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...")
        with open(local_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    total_size += len(chunk)

        print(f"âœ… ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“ ê²½ë¡œ: {local_path}")
        print(f"ğŸ“ í¬ê¸°: {total_size:,} bytes")

        # íŒŒì¼ í¬ê¸° ê²€ì¦
        if total_size < 1000:  # 1KB ë¯¸ë§Œì´ë©´ ì˜ì‹¬
            print("âš ï¸ íŒŒì¼ í¬ê¸°ê°€ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤. ë‚´ìš© í™•ì¸...")
            try:
                with open(local_path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read(200)
                    print(f"ğŸ“„ íŒŒì¼ ë‚´ìš©: {content}")
                    if any(
                        word in content.lower()
                        for word in ["error", "login", "unauthorized", "forbidden"]
                    ):
                        print("âŒ ì˜¤ë¥˜ ì‘ë‹µ íŒŒì¼ ê°ì§€")
                        return create_error_image(row_idx, "ì¸ì¦ ì˜¤ë¥˜")
            except:
                # ë°”ì´ë„ˆë¦¬ íŒŒì¼ì´ë©´ ì •ìƒì¼ ê°€ëŠ¥ì„±
                print("ğŸ” ë°”ì´ë„ˆë¦¬ íŒŒì¼ë¡œ ì¶”ì •, ì •ìƒ ì²˜ë¦¬")

        # ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ í™•ì¸
        try:
            with Image.open(local_path) as img:
                print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í™•ì¸: {img.format} {img.size} {img.mode}")
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ê²€ì¦ ì‹¤íŒ¨: {e}")
            # ì´ë¯¸ì§€ê°€ ì•„ë‹ˆì–´ë„ ì¼ë‹¨ ì§„í–‰

        return local_path

    except requests.exceptions.Timeout:
        print("âŒ íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜")
        return create_error_image(row_idx, "íƒ€ì„ì•„ì›ƒ")
    except requests.exceptions.ConnectionError:
        print("âŒ ì—°ê²° ì˜¤ë¥˜")
        return create_error_image(row_idx, "ì—°ê²° ì‹¤íŒ¨")
    except requests.exceptions.RequestException as e:
        print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return create_error_image(row_idx, f"ë„¤íŠ¸ì›Œí¬: {e}")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return create_error_image(row_idx, f"ì˜¤ë¥˜: {e}")


def create_error_image(row_idx, error_msg):
    """ì˜¤ë¥˜ ì´ë¯¸ì§€ ìƒì„±"""
    try:
        from PIL import Image, ImageDraw, ImageFont

        # ì´ë¯¸ì§€ ìƒì„±
        img = Image.new("RGB", (800, 600), color="#f8f9fa")
        draw = ImageDraw.Draw(img)

        # í°íŠ¸ ì„¤ì •
        try:
            font_title = ImageFont.truetype(
                "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 28
            )
            font_text = ImageFont.truetype(
                "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 18
            )
        except:
            try:
                font_title = ImageFont.truetype("arial.ttf", 28)
                font_text = ImageFont.truetype("arial.ttf", 18)
            except:
                font_title = ImageFont.load_default()
                font_text = ImageFont.load_default()

        # í…ìŠ¤íŠ¸ ì‘ì„±
        title = "Smore íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
        subtitle = f"Row {row_idx}"
        error_text = f"ì˜¤ë¥˜: {error_msg}"
        solution1 = "í•´ê²° ë°©ë²•:"
        solution2 = "1. Smore ë¡œê·¸ì¸ í›„ ì¿ í‚¤ ì„¤ì •"
        solution3 = "2. ìˆ˜ë™ìœ¼ë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"
        solution4 = "3. API ì ‘ê·¼ ê¶Œí•œ í™•ì¸"

        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        y_pos = 100
        draw.text((50, y_pos), title, fill="#dc3545", font=font_title)
        y_pos += 50
        draw.text((50, y_pos), subtitle, fill="#6c757d", font=font_text)
        y_pos += 40
        draw.text((50, y_pos), error_text, fill="#495057", font=font_text)
        y_pos += 60
        draw.text((50, y_pos), solution1, fill="#28a745", font=font_text)
        y_pos += 30
        draw.text((70, y_pos), solution2, fill="#6c757d", font=font_text)
        y_pos += 25
        draw.text((70, y_pos), solution3, fill="#6c757d", font=font_text)
        y_pos += 25
        draw.text((70, y_pos), solution4, fill="#6c757d", font=font_text)

        # í…Œë‘ë¦¬
        draw.rectangle([20, 20, 780, 580], outline="#dee2e6", width=3)

        # ì €ì¥
        error_path = os.path.join(DOWNLOAD_DIR, f"smore_error_row_{row_idx}.jpg")
        img.save(error_path, format="JPEG", quality=90)

        print(f"ğŸ–¼ï¸ ì˜¤ë¥˜ ì´ë¯¸ì§€ ìƒì„±: {error_path}")
        return error_path

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ëŒ€ì²´
        error_path = os.path.join(DOWNLOAD_DIR, f"smore_error_{row_idx}.txt")
        with open(error_path, "w", encoding="utf-8") as f:
            f.write(f"Smore download failed for row {row_idx}\n")
            f.write(f"Error: {error_msg}\n")
            f.write(f"Timestamp: {datetime.now()}\n")
            f.write(f"URL pattern: https://smore.im/api/form/download?fileId=...\n")
        return error_path


# â”€â”€â”€ ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def process_row_data(data, db_props, cookies, row_idx):
    """í–‰ ë°ì´í„° ì²˜ë¦¬"""
    props, image_urls = {}, []

    for hdr, val in data.items():
        if not val:
            continue

        prop = NORM_HEADER_TO_PROP.get(normalize_header(hdr))
        if not prop or prop not in db_props:
            continue

        ptype = db_props[prop]["type"]

        # íŒŒì¼/URL ì²˜ë¦¬
        if ptype in ["files", "url"] and str(val).startswith("=HYPERLINK"):
            file_prop, urls = process_file_property(val, prop, ptype, cookies, row_idx)
            if file_prop:
                props[prop] = file_prop
                image_urls.extend(urls)

        # ë‹¤ë¥¸ ì†ì„±ë“¤ ì²˜ë¦¬
        elif ptype == "date":
            date_prop = process_date_property(val)
            if date_prop:
                props[prop] = date_prop
        elif ptype == "url" and not str(val).startswith("=HYPERLINK"):
            props[prop] = {"url": str(val).strip()}
        elif ptype == "title":
            props[prop] = {"title": [{"text": {"content": str(val)}}]}
        elif ptype == "rich_text":
            props[prop] = {"rich_text": [{"text": {"content": str(val)}}]}
        elif ptype == "select":
            props[prop] = {"select": {"name": str(val).strip()}}

    return props, image_urls


def process_file_property(val, prop, ptype, cookies, row_idx):
    """íŒŒì¼ ì†ì„± ì²˜ë¦¬"""
    try:
        # HYPERLINK íŒŒì‹±
        m = re.match(r'=HYPERLINK\("([^"]+)"(?:\s*,\s*"([^"]+)")?\)', val)
        if not m:
            print(f"âŒ HYPERLINK íŒŒì‹± ì‹¤íŒ¨: {val}")
            return None, []

        url = m.group(1)
        disp = m.group(2) or f"íŒŒì¼_{row_idx}"

        print(f"ğŸ”— íŒŒì¼ URL: {url}")
        print(f"ğŸ“ í‘œì‹œëª…: {disp}")

        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        local_path = download_smore_image_direct(url, cookies, row_idx)

        # S3 ì—…ë¡œë“œ
        key = safe_key_name(row_idx, os.path.basename(local_path))
        s3_url = upload_to_s3(local_path, key)

        if ptype == "url":
            return {"url": s3_url}, [s3_url]
        else:  # files
            display_name = disp[:97] + "..." if len(disp) > 100 else disp
            return {"files": [{"name": display_name, "external": {"url": s3_url}}]}, [
                s3_url
            ]

    except Exception as e:
        print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None, []


def process_date_property(val):
    """ë‚ ì§œ ì†ì„± ì²˜ë¦¬"""
    try:
        # =DATE() í•¨ìˆ˜ íŒŒì‹±
        m = re.match(r"^=DATE\(\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*\)", str(val))
        if m:
            y, mo, da = map(int, m.groups())
            return {"date": {"start": datetime(y, mo, da).isoformat()}}

        # Excel ì‹œë¦¬ì–¼ ë‚ ì§œ ë³€í™˜
        dt = excel_serial_to_datetime(val)
        if dt:
            return {"date": {"start": dt.isoformat()}}

    except Exception as e:
        print(f"âš ï¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {e}")

    return None


def mark_row_complete(row_idx):
    """í–‰ ì™„ë£Œ í‘œì‹œ"""
    try:
        cell = f"{SHEET_NAME}!S{row_idx}"
        sheets.spreadsheets().values().update(
            spreadsheetId=SPREADSHEET_ID,
            range=cell,
            valueInputOption="USER_ENTERED",
            body={"values": [["ì™„ë£Œ"]]},
        ).execute()
        print(f"âœ… ì™„ë£Œ í‘œì‹œ: {cell}")
    except Exception as e:
        print(f"âš ï¸ ì™„ë£Œ í‘œì‹œ ì‹¤íŒ¨: {e}")


def add_image_blocks(page_id, image_urls, row_idx):
    """ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€"""
    for url in image_urls:
        try:
            ext = os.path.splitext(urlparse(url).path)[1].lower()
            block_type = "file" if ext == ".pdf" else "image"

            block = {
                "object": "block",
                "type": block_type,
                block_type: {"type": "external", "external": {"url": url}},
            }

            notion.blocks.children.append(block_id=page_id, children=[block])
            print(f"ğŸ–¼ï¸ ë¸”ë¡ ì¶”ê°€ ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ ë¸”ë¡ ì¶”ê°€ ì‹¤íŒ¨: {e}")


# â”€â”€â”€ ë©”ì¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sheet_to_notion_s3():
    """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
    print("ğŸš€ Google Sheets to Notion ì²˜ë¦¬ ì‹œì‘")

    try:
        # ì¿ í‚¤ ë¡œë“œ
        cookies = get_smore_cookies()

        # ì‹œíŠ¸ ë°ì´í„° ì½ê¸°
        print("ğŸ“Š Google Sheets ë°ì´í„° ì½ê¸°")
        res = (
            sheets.spreadsheets()
            .values()
            .get(
                spreadsheetId=SPREADSHEET_ID,
                range=SHEET_RANGE,
                valueRenderOption="FORMULA",
            )
            .execute()
        )

        rows = res.get("values", [])
        if len(rows) < 2:
            print("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        headers = [normalize_header(h) for h in rows[0]]
        done_idx = headers.index("ì´ê´€ì™„ë£Œ") if "ì´ê´€ì™„ë£Œ" in headers else None
        data_rows = rows[1:]
        recs = (
            data_rows[TEST_OFFSET : TEST_OFFSET + TEST_LIMIT]
            if TEST_LIMIT > 0
            else data_rows[TEST_OFFSET:]
        )

        # Notion DB ì†ì„± í™•ì¸
        print("ğŸ” Notion ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸")
        db_props = notion.databases.retrieve(database_id=NOTION_DB_ID)["properties"]

        print(f"ğŸ“ ì²˜ë¦¬í•  í–‰: {len(recs)}ê°œ")

        # ê° í–‰ ì²˜ë¦¬
        success_count = 0
        error_count = 0

        for i, row in enumerate(recs, start=TEST_OFFSET + 2):
            try:
                print(f"\nğŸ“‹ [Row {i}] ì²˜ë¦¬ ì‹œì‘")

                # ì™„ë£Œ ì²´í¬
                if done_idx is not None and len(row) > done_idx:
                    cell_val = row[done_idx].strip()
                    if cell_val.startswith("ì™„ë£Œ"):
                        print(f"â­ï¸ [Row {i}] ì´ë¯¸ ì²˜ë¦¬ëœ í–‰")
                        continue

                # ë°ì´í„° ì²˜ë¦¬
                data = {
                    headers[j]: row[j] if j < len(row) else ""
                    for j in range(len(headers))
                }
                props, image_urls = process_row_data(data, db_props, cookies, i)

                if props:
                    # Notion í˜ì´ì§€ ìƒì„±
                    page = notion.pages.create(
                        parent={"database_id": NOTION_DB_ID}, properties=props
                    )

                    # ì™„ë£Œ í‘œì‹œ
                    mark_row_complete(i)

                    # ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
                    if image_urls:
                        add_image_blocks(page["id"], image_urls, i)

                    success_count += 1
                    print(f"âœ… [Row {i}] ì²˜ë¦¬ ì™„ë£Œ")
                else:
                    print(f"âš ï¸ [Row {i}] ë§¤í•‘ëœ ì†ì„± ì—†ìŒ")
                    error_count += 1

            except Exception as e:
                print(f"âŒ [Row {i}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback

                traceback.print_exc()
                error_count += 1
                continue

        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  - ì„±ê³µ: {success_count}ê°œ")
        print(f"  - ì‹¤íŒ¨: {error_count}ê°œ")
        print(f"  - ì „ì²´: {len(recs)}ê°œ")

        if error_count > 0:
            print("\nğŸ’¡ ì˜¤ë¥˜ í•´ê²° ë°©ë²•:")
            print("  1. SMORE_COOKIES í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
            print("  2. AWS ìê²©ì¦ëª… í™•ì¸")
            print("  3. Smore API ì ‘ê·¼ ê¶Œí•œ í™•ì¸")

    except Exception as e:
        print(f"âŒ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    sheet_to_notion_s3()

# ================================================================
# ì¿ í‚¤ ìˆ˜ë™ ì„¤ì • ë°©ë²•:
# 1. ë¸Œë¼ìš°ì €ì—ì„œ smore.im ë¡œê·¸ì¸
# 2. F12 > Application > Cookiesì—ì„œ ëª¨ë“  ì¿ í‚¤ ë³µì‚¬
# 3. JSON í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ í™˜ê²½ë³€ìˆ˜ ì„¤ì •:
#    SMORE_COOKIES='{"PHPSESSID":"ì‹¤ì œê°’","session_id":"ì‹¤ì œê°’"}'
#
# ë˜ëŠ” ê°œë³„ í…ŒìŠ¤íŠ¸:
# curl -I "https://smore.im/api/form/download?fileId=3SJ9V2BprZQL5uuiAaGim9Sc124mEg"
# ================================================================
