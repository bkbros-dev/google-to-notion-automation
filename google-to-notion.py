# import os
# import re
# import tempfile
# import base64
# import requests
# from datetime import datetime, timedelta
# import boto3
# import chromedriver_autoinstaller
# from selenium.webdriver.chrome.service import Service
# from urllib.parse import urlparse, unquote
# from PIL import Image
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC

# from google.oauth2 import service_account
# from googleapiclient.discovery import build
# from notion_client import Client as NotionClient

# # â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (GitHub Actionsìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CHROME_BINARY = os.getenv("CHROME_BINARY", "/usr/bin/google-chrome")
# SMORE_USER_DATA = os.getenv("SMORE_USER_DATA")
# SMORE_PROFILE = os.getenv("SMORE_PROFILE")

# # Google ì¸ì¦ íŒŒì¼ ì²˜ë¦¬
# GOOGLE_CRED = os.getenv(
#     "GOOGLE_APPLICATION_CREDENTIALS", "/tmp/google_credentials.json"
# )

# SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
# SHEET_RANGE = os.getenv("SHEET_RANGE")
# AWS_REGION = os.getenv("AWS_DEFAULT_REGION")
# S3_BUCKET = os.getenv("S3_BUCKET_NAME")
# NOTION_TOKEN = os.getenv("NOTION_TOKEN")
# NOTION_DB_ID = os.getenv("NOTION_DATABASE_ID")
# TEST_OFFSET = int(os.getenv("TEST_OFFSET", "0"))
# TEST_LIMIT = int(os.getenv("TEST_LIMIT", "0"))

# # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬
# DOWNLOAD_DIR = os.getenv("SMORE_DOWNLOAD_DIR", tempfile.gettempdir())
# os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# # â”€â”€â”€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
# ga_creds = service_account.Credentials.from_service_account_file(
#     GOOGLE_CRED, scopes=SCOPES
# )
# sheets = build("sheets", "v4", credentials=ga_creds)

# s3 = boto3.client("s3", region_name=AWS_REGION)
# notion = NotionClient(auth=NOTION_TOKEN)

# SHEET_NAME = SHEET_RANGE.split("!")[0]

# # â”€â”€â”€ í—¤ë” ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HEADER_TO_PROP = {
#     "ìˆœë²ˆ": "ìˆœë²ˆ",
#     "ì¶œì‚°ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì„¸ìš” ë“±ë³¸, ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ, ì¶œìƒì¦ëª…ì„œ, ì‚°ëª¨ìˆ˜ì²© ì¤‘ íƒ1": "ì¦ëª…ì„œ ì´ë¯¸ì§€",
#     "íƒ€ê°€ëª°ì—ì„œ ì‚¬ìš© ì¤‘ì´ì‹  ì•„ì´ë””ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’› ê°„í¸ê°€ì… í•˜ì‹  ê²½ìš° ì•± ì•„ì´ë””ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.(ex.1234567@N)": "íƒ€ê°€ëª° ì•„ì´ë””",
#     "ì‹ ì²­ì ë³¸ì¸ì˜ ì„±í•¨ì„ ì ì–´ì£¼ì„¸ìš” ğŸ˜ƒ â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì´ë¦„",
#     "ğŸ“± ì—°ë½ ê°€ëŠ¥í•œ ë²ˆí˜¸ë¥¼ ì ì–´ì£¼ì„¸ìš”ğŸ’› (ì˜ˆ: 010-1234-5678)": "ì—°ë½ì²˜",
#     "ë³´ë“¬ë°•ìŠ¤ë¥¼ ë°›ìœ¼ì‹¤ ìƒì„¸ ì£¼ì†Œë¥¼ ì ì–´ ì£¼ì„¸ìš”.ğŸ’› â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì£¼ì†Œ",
#     "ğŸ“… ì•„ê¸° ì¶œìƒ ì˜ˆì •ì¼ì´ë‚˜ ì¶œìƒì¼ì„ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’›": "ì¶œìƒì¼",
#     "ë‹´ë‹¹ì": "ë‹´ë‹¹ì",
#     "ì /ë¶€": "ì /ë¶€",
# }


# def normalize_header(h: str) -> str:
#     return re.sub(r"\s+", " ", h.replace("\n", " ").strip())


# NORM_HEADER_TO_PROP = {normalize_header(k): v for k, v in HEADER_TO_PROP.items()}


# # â”€â”€â”€ Excel serial date ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def excel_serial_to_datetime(serial) -> datetime:
#     try:
#         days = float(serial)
#     except:
#         return None
#     epoch = datetime(1899, 12, 30) if days >= 61 else datetime(1899, 12, 31)
#     return epoch + timedelta(days=days)


# # â”€â”€â”€ íŒŒì¼ëª… ì•ˆì „ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def safe_key_name(row_idx: int, filename: str) -> str:
#     base, ext = os.path.splitext(filename)
#     b64 = base64.urlsafe_b64encode(base.encode("utf-8")).decode("ascii")
#     return f"row{row_idx}_{b64}{ext}"


# # â”€â”€â”€ S3 ì—…ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # def upload_to_s3(local: str, key: str) -> str:
# # import mimetypes


# # ctype, _ = mimetypes.guess_type(local)
# # s3.upload_file(
# #     Filename=local,
# #     Bucket=S3_BUCKET,
# #     Key=key,
# #     ExtraArgs={
# #         "ACL": "public-read",
# #         "ContentType": ctype or "application/octet-stream",
# #     },
# # )
# # return f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"
# # ì´ë¯¸ì§€ì¸ ê²½ìš° ì••ì¶•
# def upload_to_s3(local, key):
#     # ì´ë¯¸ì§€ ì••ì¶• ë° ë¦¬ì‚¬ì´ì¦ˆ ì‹œë„
#     try:
#         from PIL import Image

#         img = Image.open(local)
#         # ìµœëŒ€ ê°€ë¡œ/ì„¸ë¡œ 1024pxë¡œ ë¦¬ì‚¬ì´ì¦ˆ
#         resample = getattr(Image, "Resampling", Image).LANCZOS
#         img.thumbnail((1024, 1024), resample)
#         ext = os.path.splitext(local)[1].lower()
#         quality = 75  # ìˆ˜ì •: í’ˆì§ˆ ë‚®ì¶° ìš©ëŸ‰ ì¤„ì´ê¸°
#         if ext in (".jpg", ".jpeg"):
#             img.save(local, format="JPEG", optimize=True, quality=quality)
#         else:
#             # PNG ë“±ì€ JPEGë¡œ ë³€í™˜
#             rgb = img.convert("RGB")
#             jpeg_path = os.path.splitext(local)[0] + ".jpg"
#             rgb.save(jpeg_path, format="JPEG", optimize=True, quality=quality)
#             local = jpeg_path
#     except ImportError:
#         # PIL ë¯¸ì„¤ì¹˜ ì‹œ ì••ì¶• ìƒëµ
#         pass
#     except Exception as e:
#         print(f"[ì••ì¶• ì˜¤ë¥˜] {local}: {e}")
#     # S3 ì—…ë¡œë“œ
#     import mimetypes

#     ctype, _ = mimetypes.guess_type(local)
#     s3.upload_file(
#         Filename=local,
#         Bucket=S3_BUCKET,
#         Key=key,
#         ExtraArgs={
#             "ACL": "public-read",
#             "ContentType": ctype or "application/octet-stream",
#         },
#     )
#     return f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"


# def get_smore_cookies():
#     driver_path = chromedriver_autoinstaller.install()
#     opts = Options()
#     opts.binary_location = CHROME_BINARY

#     # GitHub Actions í™˜ê²½ìš© ì˜µì…˜ ì¶”ê°€
#     opts.add_argument("--headless")
#     opts.add_argument("--no-sandbox")
#     opts.add_argument("--disable-dev-shm-usage")
#     opts.add_argument("--disable-gpu")
#     opts.add_argument("--remote-debugging-port=9222")

#     # ê¸°ì¡´ ì˜µì…˜
#     if SMORE_USER_DATA:
#         opts.add_argument(f"--user-data-dir={SMORE_USER_DATA}")
#     if SMORE_PROFILE:
#         opts.add_argument(f"--profile-directory={SMORE_PROFILE}")

#     driver = webdriver.Chrome(service=Service(driver_path), options=opts)
#     try:
#         driver.get("https://smore.im")
#         return {c["name"]: c["value"] for c in driver.get_cookies()}
#     finally:
#         driver.quit()


# def download_smore_image(page_url, cookies, row_idx):
#     driver_path = chromedriver_autoinstaller.install()
#     opts = Options()
#     opts.binary_location = CHROME_BINARY
#     opts.add_argument(f"--user-data-dir={SMORE_USER_DATA}")
#     opts.add_argument(f"--profile-directory={SMORE_PROFILE}")
#     driver = webdriver.Chrome(service=Service(driver_path), options=opts)
#     try:
#         driver.get(page_url)
#         link = WebDriverWait(driver, 15).until(
#             EC.element_to_be_clickable((By.ID, "download"))
#         )
#         file_url = link.get_attribute("href")
#         sess = requests.Session()
#         sess.cookies.update(cookies)
#         resp = sess.get(file_url, stream=True, timeout=30)
#     finally:
#         driver.quit()

#     resp.raise_for_status()
#     if "text/html" in resp.headers.get("Content-Type", ""):
#         raise RuntimeError("HTML ë°˜í™˜ë¨(ë¡œê·¸ì¸ í•„ìš”?): " + page_url)

#     cd = resp.headers.get("content-disposition", "")
#     m = re.search(r"filename\*=UTF-8''([^;]+)", cd) or re.search(
#         r"filename=?\"?([^\";]+)\"?", cd
#     )
#     orig = unquote(m.group(1)) if m else os.path.basename(urlparse(file_url).path)
#     filename = safe_key_name(row_idx, orig)
#     local = os.path.join(DOWNLOAD_DIR, filename)
#     with open(local, "wb") as f:
#         for chunk in resp.iter_content(1024):
#             f.write(chunk)
#     return local


# # â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def sheet_to_notion_s3():
#     cookies = get_smore_cookies()
#     res = (
#         sheets.spreadsheets()
#         .values()
#         .get(
#             spreadsheetId=SPREADSHEET_ID, range=SHEET_RANGE, valueRenderOption="FORMULA"
#         )
#         .execute()
#     )
#     rows = res.get("values", [])
#     if len(rows) < 2:
#         print("ë°ì´í„° ì—†ìŒ")
#         return

#     headers = [normalize_header(h) for h in rows[0]]
#     done_idx = headers.index("ì´ê´€ì™„ë£Œ") if "ì´ê´€ì™„ë£Œ" in headers else None
#     data_rows = rows[1:]
#     recs = data_rows[TEST_OFFSET : TEST_OFFSET + TEST_LIMIT]
#     db_props = notion.databases.retrieve(database_id=NOTION_DB_ID)["properties"]

#     for i, row in enumerate(recs, start=TEST_OFFSET + 2):
#         # â”€â”€â”€ ì—¬ê¸°ì„œ Sì—´ 'ì™„ë£Œ' ì²´í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#         if done_idx is not None and len(row) > done_idx:
#             cell_val = row[done_idx].strip()
#             # 'ì™„ë£Œ'ë¡œ ì‹œì‘(ì˜ˆ: "ì™„ë£Œ", "ì™„ë£Œ<")í•˜ë©´ ê±´ë„ˆëœë‹ˆë‹¤
#             if cell_val.startswith("ì™„ë£Œ"):
#                 print(f"[Row {i}] ì´ë¯¸ ì™„ë£Œëœ í–‰, ê±´ë„ˆëœë‹ˆë‹¤")
#                 continue
#         data = {headers[j]: row[j] if j < len(row) else "" for j in range(len(headers))}
#         props, image_urls = {}, []

#         for hdr, val in data.items():
#             if not val:
#                 continue
#             prop = NORM_HEADER_TO_PROP.get(normalize_header(hdr))
#             if not prop or prop not in db_props:
#                 continue
#             ptype = db_props[prop]["type"]
#             if ptype == "files":
#                 # m = re.match(r'=HYPERLINK\("([^"\)]+)"', val)
#                 # url = m.group(1) if m else val
#                 m = re.match(r'=HYPERLINK\("([^"]+)"(?:\s*,\s*"([^"]+)")?\)', val)
#                 url = m.group(1)  # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL
#                 disp = m.group(2) if m and m.group(2) else None  # ì˜µì…˜ í‘œì‹œëª…
#                 try:
#                     local = download_smore_image(url, cookies, i)
#                     key = safe_key_name(i, os.path.basename(local))
#                     s3_url = upload_to_s3(local, key)
#                     original_name = disp or os.path.basename(local)
#                     if len(original_name) > 100:
#                         display_name = original_name[:97] + "..."
#                     else:
#                         display_name = original_name
#                     props[prop] = {
#                         "files": [
#                             {
#                                 # "name": os.path.basename(local),
#                                 "name": display_name,
#                                 "external": {"url": s3_url},
#                             }
#                         ]
#                     }
#                     image_urls.append(s3_url)
#                 except Exception as e:
#                     print(f"[Row {i}] íŒŒì¼ ì˜¤ë¥˜: {e}")
#                 continue
#             if ptype == "date":
#                 m2 = re.match(
#                     r"^=DATE\(\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*\)", str(val)
#                 )
#                 if m2:
#                     y, mo, da = map(int, m2.groups())
#                     props[prop] = {"date": {"start": datetime(y, mo, da).isoformat()}}
#                 else:
#                     dt = excel_serial_to_datetime(val)
#                     if dt:
#                         props[prop] = {"date": {"start": dt.isoformat()}}
#                 continue
#             if ptype == "url":
#                 props[prop] = {"url": val.strip()}
#                 continue
#             if ptype == "title":
#                 props[prop] = {"title": [{"text": {"content": str(val)}}]}
#                 continue
#             if ptype == "rich_text":
#                 props[prop] = {"rich_text": [{"text": {"content": str(val)}}]}
#                 continue

#         if props:
#             # print(f"[Row {i}] props=", props)  # debug
#             page = notion.pages.create(
#                 parent={"database_id": NOTION_DB_ID}, properties=props
#             )
#             print(f"[Row {i}] Notion ë“±ë¡ ì™„ë£Œ", list(props.keys()))

#             # ìˆ˜ì •: Google Sheets Sì—´ì— 'ì™„ë£Œ' í‘œì‹œ
#             cell = f"{SHEET_NAME}!S{i}"
#             sheets.spreadsheets().values().update(
#                 spreadsheetId=SPREADSHEET_ID,
#                 range=cell,
#                 valueInputOption="USER_ENTERED",
#                 body={"values": [["ì™„ë£Œ"]]},
#             ).execute()
#             print(f"[Row {i}] ì‹œíŠ¸ì— â€˜ì™„ë£Œâ€™ í‘œê¸° ì™„ë£Œ â†’ {cell}")

#             # â”€â”€â”€ ë¸”ë¡ ì‚½ì… (ì˜¤ë¥˜ ë‚˜ë©´ ê±´ë„ˆë›°ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#             for url in image_urls:
#                 ext = os.path.splitext(urlparse(url).path)[1].lower()
#                 if ext == ".pdf":
#                     block = {
#                         "object": "block",
#                         "type": "file",
#                         "file": {"type": "external", "external": {"url": url}},
#                     }
#                 else:
#                     block = {
#                         "object": "block",
#                         "type": "image",
#                         "image": {"type": "external", "external": {"url": url}},
#                     }

#                 try:
#                     notion.blocks.children.append(block_id=page["id"], children=[block])
#                     print(f"[Row {i}] ë¸”ë¡ ì‚½ì… ì™„ë£Œ â†’ {url}")
#                 except Exception as e:
#                     print(f"[Row {i}] ë¸”ë¡ ì‚½ì… ì˜¤ë¥˜, ê±´ë„ˆëœë‹ˆë‹¤: {url} â–¶ {e}")

#         else:
#             print(f"[Row {i}] ë§¤í•‘ëœ ì†ì„± ì—†ìŒ")


# if __name__ == "__main__":
#     sheet_to_notion_s3()

import os
import re
import tempfile
import base64
import requests
from datetime import datetime, timedelta
import boto3
import chromedriver_autoinstaller
from selenium.webdriver.chrome.service import Service
from dotenv import load_dotenv
from urllib.parse import urlparse, unquote
from PIL import Image
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from google.oauth2 import service_account
from googleapiclient.discovery import build
from notion_client import Client as NotionClient

# â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()


# ì›¹ ë°°í¬ í™˜ê²½ì„ ìœ„í•œ ê¸°ë³¸ê°’ ì„¤ì •
def get_env_with_default(key, default=None):
    """í™˜ê²½ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ë˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©"""
    value = os.getenv(key, default)
    if not value and key in ["CHROME_BINARY", "SMORE_USER_DATA", "SMORE_PROFILE"]:
        print(f"âš ï¸ {key} í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©.")
    return value


# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì›¹ ë°°í¬ í™˜ê²½ ëŒ€ì‘)
CHROME_BINARY = get_env_with_default("CHROME_BINARY")
SMORE_USER_DATA = get_env_with_default("SMORE_USER_DATA")
SMORE_PROFILE = get_env_with_default("SMORE_PROFILE", "Default")
GOOGLE_CRED = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
SHEET_RANGE = os.getenv("SHEET_RANGE")
AWS_REGION = os.getenv("AWS_DEFAULT_REGION", "ap-northeast-2")
S3_BUCKET = os.getenv("S3_BUCKET_NAME")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
NOTION_DB_ID = os.getenv("NOTION_DATABASE_ID")
TEST_OFFSET = int(os.getenv("TEST_OFFSET", "0"))
TEST_LIMIT = int(os.getenv("TEST_LIMIT", "0"))

# ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ (ì›¹ í™˜ê²½ ëŒ€ì‘)
DOWNLOAD_DIR = os.getenv("SMORE_DOWNLOAD_DIR", tempfile.gettempdir())
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

print(f"ğŸ”§ í™˜ê²½ ì„¤ì •:")
print(f"  - Chrome Binary: {CHROME_BINARY}")
print(f"  - User Data: {SMORE_USER_DATA}")
print(f"  - Download Dir: {DOWNLOAD_DIR}")
print(f"  - S3 Bucket: {S3_BUCKET}")

# â”€â”€â”€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# Google Credentials ì²˜ë¦¬ (ì›¹ í™˜ê²½ ëŒ€ì‘)
if GOOGLE_CRED and os.path.exists(GOOGLE_CRED):
    ga_creds = service_account.Credentials.from_service_account_file(
        GOOGLE_CRED, scopes=SCOPES
    )
else:
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ JSON ì§ì ‘ ì½ê¸° ì‹œë„
    cred_json = os.getenv("GOOGLE_CREDENTIALS_JSON")
    if cred_json:
        import json

        ga_creds = service_account.Credentials.from_service_account_info(
            json.loads(cred_json), scopes=SCOPES
        )
    else:
        raise ValueError("Google Credentialsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

sheets = build("sheets", "v4", credentials=ga_creds)
s3 = boto3.client("s3", region_name=AWS_REGION)
notion = NotionClient(auth=NOTION_TOKEN)

SHEET_NAME = SHEET_RANGE.split("!")[0]

# â”€â”€â”€ í—¤ë” ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADER_TO_PROP = {
    "ìˆœë²ˆ": "ìˆœë²ˆ",
    "ì¶œì‚°ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì„¸ìš” ë“±ë³¸, ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ, ì¶œìƒì¦ëª…ì„œ, ì‚°ëª¨ìˆ˜ì²© ì¤‘ íƒ1": "ì¦ëª…ì„œ ì´ë¯¸ì§€",
    "íƒ€ê°€ëª°ì—ì„œ ì‚¬ìš© ì¤‘ì´ì‹  ì•„ì´ë””ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’› ê°„í¸ê°€ì… í•˜ì‹  ê²½ìš° ì•± ì•„ì´ë””ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.(ex.1234567@N)": "íƒ€ê°€ëª° ì•„ì´ë””",
    "ì‹ ì²­ì ë³¸ì¸ì˜ ì„±í•¨ì„ ì ì–´ì£¼ì„¸ìš” ğŸ˜ƒ â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì´ë¦„",
    "ğŸ“± ì—°ë½ ê°€ëŠ¥í•œ ë²ˆí˜¸ë¥¼ ì ì–´ì£¼ì„¸ìš”ğŸ’› (ì˜ˆ: 010-1234-5678)": "ì—°ë½ì²˜",
    "ë³´ë“¬ë°•ìŠ¤ë¥¼ ë°›ìœ¼ì‹¤ ìƒì„¸ ì£¼ì†Œë¥¼ ì ì–´ ì£¼ì„¸ìš”.ğŸ’› â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì£¼ì†Œ",
    "ğŸ“… ì•„ê¸° ì¶œìƒ ì˜ˆì •ì¼ì´ë‚˜ ì¶œìƒì¼ì„ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’›": "ì¶œìƒì¼",
    "ë‹´ë‹¹ì": "ë‹´ë‹¹ì",
    "ì /ë¶€": "ì /ë¶€",
}


def normalize_header(h: str) -> str:
    return re.sub(r"\s+", " ", h.replace("\n", " ").strip())


NORM_HEADER_TO_PROP = {normalize_header(k): v for k, v in HEADER_TO_PROP.items()}


# â”€â”€â”€ Chrome ì˜µì…˜ ì„¤ì • (ì›¹ ë°°í¬ í™˜ê²½ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_chrome_options():
    """ì›¹ ë°°í¬ í™˜ê²½ì— ìµœì í™”ëœ Chrome ì˜µì…˜"""
    opts = Options()

    # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ì›¹ í™˜ê²½ í•„ìˆ˜)
    opts.add_argument("--headless=new")  # ìµœì‹  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ

    # ì›¹ ë°°í¬ í™˜ê²½ ìµœì í™” ì˜µì…˜
    opts.add_argument("--no-sandbox")  # ê¶Œí•œ ë¬¸ì œ í•´ê²°
    opts.add_argument("--disable-dev-shm-usage")  # ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°
    opts.add_argument("--disable-gpu")  # GPU ë¹„í™œì„±í™”
    opts.add_argument("--disable-extensions")  # í™•ì¥ ê¸°ëŠ¥ ë¹„í™œì„±í™”
    opts.add_argument("--disable-web-security")  # ë³´ì•ˆ ì œí•œ ì™„í™”
    opts.add_argument("--allow-running-insecure-content")
    opts.add_argument("--disable-background-timer-throttling")
    opts.add_argument("--disable-backgrounding-occluded-windows")
    opts.add_argument("--disable-renderer-backgrounding")
    opts.add_argument("--disable-features=TranslateUI")
    opts.add_argument("--disable-ipc-flooding-protection")

    # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì • (ë´‡ íƒì§€ ìš°íšŒ)
    opts.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    # ì°½ í¬ê¸° ì„¤ì •
    opts.add_argument("--window-size=1920,1080")

    # Chrome ë°”ì´ë„ˆë¦¬ ìœ„ì¹˜ ì„¤ì • (ìˆëŠ” ê²½ìš°)
    if CHROME_BINARY and os.path.exists(CHROME_BINARY):
        opts.binary_location = CHROME_BINARY

    # ì‚¬ìš©ì ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì • (ìˆê³  ì“°ê¸° ê°€ëŠ¥í•œ ê²½ìš°)
    if SMORE_USER_DATA and os.path.exists(SMORE_USER_DATA):
        try:
            # ì“°ê¸° ê¶Œí•œ í™•ì¸
            test_file = os.path.join(SMORE_USER_DATA, "test_write")
            with open(test_file, "w") as f:
                f.write("test")
            os.remove(test_file)
            opts.add_argument(f"--user-data-dir={SMORE_USER_DATA}")
            if SMORE_PROFILE:
                opts.add_argument(f"--profile-directory={SMORE_PROFILE}")
        except (PermissionError, OSError):
            print(f"âš ï¸ ì‚¬ìš©ì ë°ì´í„° ë””ë ‰í† ë¦¬ì— ì“°ê¸° ê¶Œí•œ ì—†ìŒ: {SMORE_USER_DATA}")

    return opts


# â”€â”€â”€ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_chrome_driver():
    """ì›¹ ë°°í¬ í™˜ê²½ìš© Chrome ë“œë¼ì´ë²„ ìƒì„±"""
    try:
        # ChromeDriver ìë™ ì„¤ì¹˜
        driver_path = chromedriver_autoinstaller.install()
        print(f"âœ… ChromeDriver ê²½ë¡œ: {driver_path}")

        # Chrome ì˜µì…˜ ì„¤ì •
        options = get_chrome_options()

        # ì„œë¹„ìŠ¤ ì„¤ì •
        service = Service(driver_path)

        # ë“œë¼ì´ë²„ ìƒì„±
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(30)  # íƒ€ì„ì•„ì›ƒ ì„¤ì •

        return driver

    except Exception as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€ ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def excel_serial_to_datetime(serial) -> datetime:
    try:
        days = float(serial)
    except:
        return None
    epoch = datetime(1899, 12, 30) if days >= 61 else datetime(1899, 12, 31)
    return epoch + timedelta(days=days)


def safe_key_name(row_idx: int, filename: str) -> str:
    base, ext = os.path.splitext(filename)
    b64 = base64.urlsafe_b64encode(base.encode("utf-8")).decode("ascii")
    return f"row{row_idx}_{b64}{ext}"


# â”€â”€â”€ S3 ì—…ë¡œë“œ í•¨ìˆ˜ (ê°œì„ ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def upload_to_s3(local, key):
    """ì´ë¯¸ì§€ ì••ì¶• ë° S3 ì—…ë¡œë“œ"""
    print(f"ğŸ“¤ S3 ì—…ë¡œë“œ ì‹œì‘: {key}")

    if not os.path.exists(local):
        raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {local}")

    compressed_path = local

    # ì´ë¯¸ì§€ ì••ì¶•
    try:
        with Image.open(local) as img:
            resample = getattr(Image, "Resampling", Image).LANCZOS
            img.thumbnail((1024, 1024), resample)

            ext = os.path.splitext(local)[1].lower()
            if ext in (".jpg", ".jpeg"):
                img.save(local, format="JPEG", optimize=True, quality=75)
            else:
                rgb = img.convert("RGB")
                jpeg_path = os.path.splitext(local)[0] + ".jpg"
                rgb.save(jpeg_path, format="JPEG", optimize=True, quality=75)
                compressed_path = jpeg_path
                key = os.path.splitext(key)[0] + ".jpg"

                try:
                    os.remove(local)
                except:
                    pass

        print(f"ğŸ—œï¸ ì´ë¯¸ì§€ ì••ì¶• ì™„ë£Œ")

    except Exception as e:
        print(f"âš ï¸ ì••ì¶• ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")

    # S3 ì—…ë¡œë“œ
    try:
        import mimetypes

        ctype, _ = mimetypes.guess_type(compressed_path)

        s3.upload_file(
            Filename=compressed_path,
            Bucket=S3_BUCKET,
            Key=key,
            ExtraArgs={
                "ACL": "public-read",
                "ContentType": ctype or "application/octet-stream",
            },
        )

        s3_url = f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"
        print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_url}")

        # ë¡œì»¬ íŒŒì¼ ì •ë¦¬
        try:
            os.remove(compressed_path)
        except:
            pass

        return s3_url

    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€ Smore ì¿ í‚¤ íšë“ (ì›¹ í™˜ê²½ ëŒ€ì‘) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_smore_cookies():
    """ì›¹ ë°°í¬ í™˜ê²½ìš© ì¿ í‚¤ íšë“"""
    print("ğŸª Smore ì¿ í‚¤ íšë“ ì‹œì‘")
    driver = None

    try:
        driver = get_chrome_driver()
        driver.get("https://smore.im")

        # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        cookies = {c["name"]: c["value"] for c in driver.get_cookies()}
        print(f"âœ… ì¿ í‚¤ íšë“ ì™„ë£Œ: {len(cookies)}ê°œ")
        return cookies

    except Exception as e:
        print(f"âŒ ì¿ í‚¤ íšë“ ì‹¤íŒ¨: {e}")
        return {}
    finally:
        if driver:
            driver.quit()


# â”€â”€â”€ Smore íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì›¹ í™˜ê²½ ëŒ€ì‘) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_smore_image(page_url, cookies, row_idx):
    """ì›¹ ë°°í¬ í™˜ê²½ìš© íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    print(f"â¬‡ï¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: Row {row_idx}")
    driver = None

    try:
        driver = get_chrome_driver()

        # ì¿ í‚¤ ì„¤ì •ì„ ìœ„í•´ ë¨¼ì € smore.im ë°©ë¬¸
        driver.get("https://smore.im")
        for name, value in cookies.items():
            try:
                driver.add_cookie({"name": name, "value": value})
            except:
                pass

        # ì‹¤ì œ í˜ì´ì§€ ë°©ë¬¸
        driver.get(page_url)

        # ë‹¤ìš´ë¡œë“œ ë§í¬ ëŒ€ê¸° ë° í´ë¦­
        link = WebDriverWait(driver, 15).until(
            EC.element_to_be_clickable((By.ID, "download"))
        )
        file_url = link.get_attribute("href")

        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        sess = requests.Session()
        sess.cookies.update(cookies)

        # í—¤ë” ì„¤ì • (ë´‡ íƒì§€ ìš°íšŒ)
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        resp = sess.get(file_url, stream=True, timeout=30, headers=headers)
        resp.raise_for_status()

        if "text/html" in resp.headers.get("Content-Type", ""):
            raise RuntimeError("HTML ë°˜í™˜ë¨ (ë¡œê·¸ì¸ ì‹¤íŒ¨?)")

        # íŒŒì¼ëª… ì¶”ì¶œ
        cd = resp.headers.get("content-disposition", "")
        m = re.search(r"filename\*=UTF-8''([^;]+)", cd) or re.search(
            r"filename=?\"?([^\";]+)\"?", cd
        )
        orig = unquote(m.group(1)) if m else f"file_{row_idx}.jpg"

        filename = safe_key_name(row_idx, orig)
        local = os.path.join(DOWNLOAD_DIR, filename)

        # íŒŒì¼ ì €ì¥
        with open(local, "wb") as f:
            for chunk in resp.iter_content(1024):
                f.write(chunk)

        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local}")
        return local

    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    finally:
        if driver:
            driver.quit()


# â”€â”€â”€ ë©”ì¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sheet_to_notion_s3():
    """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
    print("ğŸš€ Google Sheets to Notion ì²˜ë¦¬ ì‹œì‘")

    try:
        # ì¿ í‚¤ íšë“
        cookies = get_smore_cookies()
        if not cookies:
            print("âš ï¸ ì¿ í‚¤ íšë“ ì‹¤íŒ¨, ê³„ì† ì§„í–‰")

        # ì‹œíŠ¸ ë°ì´í„° ì½ê¸°
        print("ğŸ“Š Google Sheets ë°ì´í„° ì½ê¸°")
        res = (
            sheets.spreadsheets()
            .values()
            .get(
                spreadsheetId=SPREADSHEET_ID,
                range=SHEET_RANGE,
                valueRenderOption="FORMULA",
            )
            .execute()
        )

        rows = res.get("values", [])
        if len(rows) < 2:
            print("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        headers = [normalize_header(h) for h in rows[0]]
        done_idx = headers.index("ì´ê´€ì™„ë£Œ") if "ì´ê´€ì™„ë£Œ" in headers else None
        data_rows = rows[1:]
        recs = (
            data_rows[TEST_OFFSET : TEST_OFFSET + TEST_LIMIT]
            if TEST_LIMIT > 0
            else data_rows[TEST_OFFSET:]
        )

        # Notion DB ì†ì„± í™•ì¸
        print("ğŸ” Notion ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸")
        db_props = notion.databases.retrieve(database_id=NOTION_DB_ID)["properties"]

        print(f"ğŸ“ ì²˜ë¦¬í•  í–‰: {len(recs)}ê°œ")

        # ê° í–‰ ì²˜ë¦¬
        success_count = 0
        for i, row in enumerate(recs, start=TEST_OFFSET + 2):
            try:
                print(f"\nğŸ“‹ [Row {i}] ì²˜ë¦¬ ì‹œì‘")

                # ì™„ë£Œ ì²´í¬
                if done_idx is not None and len(row) > done_idx:
                    cell_val = row[done_idx].strip()
                    if cell_val.startswith("ì™„ë£Œ"):
                        print(f"â­ï¸ [Row {i}] ì´ë¯¸ ì²˜ë¦¬ëœ í–‰")
                        continue

                # ë°ì´í„° ì²˜ë¦¬
                data = {
                    headers[j]: row[j] if j < len(row) else ""
                    for j in range(len(headers))
                }
                props, image_urls = process_row_data(data, db_props, cookies, i)

                if props:
                    # Notion í˜ì´ì§€ ìƒì„±
                    page = notion.pages.create(
                        parent={"database_id": NOTION_DB_ID}, properties=props
                    )

                    # ì™„ë£Œ í‘œì‹œ
                    mark_row_complete(i)

                    # ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€
                    add_image_blocks(page["id"], image_urls, i)

                    success_count += 1
                    print(f"âœ… [Row {i}] ì²˜ë¦¬ ì™„ë£Œ")
                else:
                    print(f"âš ï¸ [Row {i}] ë§¤í•‘ëœ ì†ì„± ì—†ìŒ")

            except Exception as e:
                print(f"âŒ [Row {i}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(recs)}ê°œ ì„±ê³µ")

    except Exception as e:
        print(f"âŒ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


def process_row_data(data, db_props, cookies, row_idx):
    """í–‰ ë°ì´í„° ì²˜ë¦¬"""
    props, image_urls = {}, []

    for hdr, val in data.items():
        if not val:
            continue

        prop = NORM_HEADER_TO_PROP.get(normalize_header(hdr))
        if not prop or prop not in db_props:
            continue

        ptype = db_props[prop]["type"]

        # íŒŒì¼/URL ì²˜ë¦¬
        if ptype in ["files", "url"] and str(val).startswith("=HYPERLINK"):
            file_prop, urls = process_file_property(val, prop, ptype, cookies, row_idx)
            if file_prop:
                props[prop] = file_prop
                image_urls.extend(urls)

        # ë‹¤ë¥¸ ì†ì„±ë“¤ ì²˜ë¦¬
        elif ptype == "date":
            date_prop = process_date_property(val)
            if date_prop:
                props[prop] = date_prop
        elif ptype == "url" and not str(val).startswith("=HYPERLINK"):
            props[prop] = {"url": str(val).strip()}
        elif ptype == "title":
            props[prop] = {"title": [{"text": {"content": str(val)}}]}
        elif ptype == "rich_text":
            props[prop] = {"rich_text": [{"text": {"content": str(val)}}]}

    return props, image_urls


def process_file_property(val, prop, ptype, cookies, row_idx):
    """íŒŒì¼ ì†ì„± ì²˜ë¦¬"""
    try:
        m = re.match(r'=HYPERLINK\("([^"]+)"(?:\s*,\s*"([^"]+)")?\)', val)
        if not m:
            return None, []

        url = m.group(1)
        disp = m.group(2) or "íŒŒì¼"

        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° S3 ì—…ë¡œë“œ
        local = download_smore_image(url, cookies, row_idx)
        key = safe_key_name(row_idx, os.path.basename(local))
        s3_url = upload_to_s3(local, key)

        if ptype == "url":
            return {"url": s3_url}, [s3_url]
        else:  # files
            display_name = disp[:97] + "..." if len(disp) > 100 else disp
            return {"files": [{"name": display_name, "external": {"url": s3_url}}]}, [
                s3_url
            ]

    except Exception as e:
        print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None, []


def process_date_property(val):
    """ë‚ ì§œ ì†ì„± ì²˜ë¦¬"""
    try:
        m = re.match(r"^=DATE\(\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*\)", str(val))
        if m:
            y, mo, da = map(int, m.groups())
            return {"date": {"start": datetime(y, mo, da).isoformat()}}
        else:
            dt = excel_serial_to_datetime(val)
            if dt:
                return {"date": {"start": dt.isoformat()}}
    except:
        pass
    return None


def mark_row_complete(row_idx):
    """í–‰ ì™„ë£Œ í‘œì‹œ"""
    try:
        cell = f"{SHEET_NAME}!S{row_idx}"
        sheets.spreadsheets().values().update(
            spreadsheetId=SPREADSHEET_ID,
            range=cell,
            valueInputOption="USER_ENTERED",
            body={"values": [["ì™„ë£Œ"]]},
        ).execute()
    except Exception as e:
        print(f"âš ï¸ ì™„ë£Œ í‘œì‹œ ì‹¤íŒ¨: {e}")


def add_image_blocks(page_id, image_urls, row_idx):
    """ì´ë¯¸ì§€ ë¸”ë¡ ì¶”ê°€"""
    for url in image_urls:
        try:
            ext = os.path.splitext(urlparse(url).path)[1].lower()
            block = {
                "object": "block",
                "type": "file" if ext == ".pdf" else "image",
                (ext == ".pdf" and "file" or "image"): {
                    "type": "external",
                    "external": {"url": url},
                },
            }
            notion.blocks.children.append(block_id=page_id, children=[block])
        except Exception as e:
            print(f"âš ï¸ ë¸”ë¡ ì¶”ê°€ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    sheet_to_notion_s3()
# $Env:TEST_OFFSET = "1"; $Env:TEST_LIMIT = "10"; python google-to-notion.py
