import os
import re
import tempfile
import base64
import requests
from datetime import datetime, timedelta
import boto3
import chromedriver_autoinstaller
from selenium.webdriver.chrome.service import Service
from urllib.parse import urlparse, unquote
from PIL import Image
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from google.oauth2 import service_account
from googleapiclient.discovery import build
from notion_client import Client as NotionClient

# â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (GitHub Actionsìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHROME_BINARY = os.getenv("CHROME_BINARY", "/usr/bin/google-chrome")
SMORE_USER_DATA = os.getenv("SMORE_USER_DATA")
SMORE_PROFILE = os.getenv("SMORE_PROFILE")

# Google ì¸ì¦ íŒŒì¼ ì²˜ë¦¬
GOOGLE_CRED = os.getenv(
    "GOOGLE_APPLICATION_CREDENTIALS", "/tmp/google_credentials.json"
)

SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
SHEET_RANGE = os.getenv("SHEET_RANGE")
AWS_REGION = os.getenv("AWS_DEFAULT_REGION")
S3_BUCKET = os.getenv("S3_BUCKET_NAME")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
NOTION_DB_ID = os.getenv("NOTION_DATABASE_ID")
TEST_OFFSET = int(os.getenv("TEST_OFFSET", "0"))
TEST_LIMIT = int(os.getenv("TEST_LIMIT", "0"))

# ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬
DOWNLOAD_DIR = os.getenv("SMORE_DOWNLOAD_DIR", tempfile.gettempdir())
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# â”€â”€â”€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
ga_creds = service_account.Credentials.from_service_account_file(
    GOOGLE_CRED, scopes=SCOPES
)
sheets = build("sheets", "v4", credentials=ga_creds)

s3 = boto3.client("s3", region_name=AWS_REGION)
notion = NotionClient(auth=NOTION_TOKEN)

SHEET_NAME = SHEET_RANGE.split("!")[0]

# â”€â”€â”€ í—¤ë” ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADER_TO_PROP = {
    "ìˆœë²ˆ": "ìˆœë²ˆ",
    "ì¶œì‚°ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì„¸ìš” ë“±ë³¸, ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ, ì¶œìƒì¦ëª…ì„œ, ì‚°ëª¨ìˆ˜ì²© ì¤‘ íƒ1": "ì¦ëª…ì„œ ì´ë¯¸ì§€",
    "íƒ€ê°€ëª°ì—ì„œ ì‚¬ìš© ì¤‘ì´ì‹  ì•„ì´ë””ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’› ê°„í¸ê°€ì… í•˜ì‹  ê²½ìš° ì•± ì•„ì´ë””ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.(ex.1234567@N)": "íƒ€ê°€ëª° ì•„ì´ë””",
    "ì‹ ì²­ì ë³¸ì¸ì˜ ì„±í•¨ì„ ì ì–´ì£¼ì„¸ìš” ğŸ˜ƒ â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì´ë¦„",
    "ğŸ“± ì—°ë½ ê°€ëŠ¥í•œ ë²ˆí˜¸ë¥¼ ì ì–´ì£¼ì„¸ìš”ğŸ’› (ì˜ˆ: 010-1234-5678)": "ì—°ë½ì²˜",
    "ë³´ë“¬ë°•ìŠ¤ë¥¼ ë°›ìœ¼ì‹¤ ìƒì„¸ ì£¼ì†Œë¥¼ ì ì–´ ì£¼ì„¸ìš”.ğŸ’› â€» í‘œê¸° ì˜¤ë¥˜ë¡œ ì¸í•œ ëŒ€ìƒ ì œì™¸ ë° ê²½í’ˆ ë¯¸ìˆ˜ë ¹ì‹œ ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.": "ì£¼ì†Œ",
    "ğŸ“… ì•„ê¸° ì¶œìƒ ì˜ˆì •ì¼ì´ë‚˜ ì¶œìƒì¼ì„ ì•Œë ¤ì£¼ì„¸ìš”ğŸ’›": "ì¶œìƒì¼",
    "ë‹´ë‹¹ì": "ë‹´ë‹¹ì",
    "ì /ë¶€": "ì /ë¶€",
}


def normalize_header(h: str) -> str:
    return re.sub(r"\s+", " ", h.replace("\n", " ").strip())


NORM_HEADER_TO_PROP = {normalize_header(k): v for k, v in HEADER_TO_PROP.items()}


# â”€â”€â”€ Excel serial date ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def excel_serial_to_datetime(serial) -> datetime:
    try:
        days = float(serial)
    except:
        return None
    epoch = datetime(1899, 12, 30) if days >= 61 else datetime(1899, 12, 31)
    return epoch + timedelta(days=days)


# â”€â”€â”€ íŒŒì¼ëª… ì•ˆì „ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_key_name(row_idx: int, filename: str) -> str:
    base, ext = os.path.splitext(filename)
    b64 = base64.urlsafe_b64encode(base.encode("utf-8")).decode("ascii")
    return f"row{row_idx}_{b64}{ext}"


# â”€â”€â”€ S3 ì—…ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def upload_to_s3(local: str, key: str) -> str:
# import mimetypes


# ctype, _ = mimetypes.guess_type(local)
# s3.upload_file(
#     Filename=local,
#     Bucket=S3_BUCKET,
#     Key=key,
#     ExtraArgs={
#         "ACL": "public-read",
#         "ContentType": ctype or "application/octet-stream",
#     },
# )
# return f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"
# ì´ë¯¸ì§€ì¸ ê²½ìš° ì••ì¶•
def upload_to_s3(local, key):
    # ì´ë¯¸ì§€ ì••ì¶• ë° ë¦¬ì‚¬ì´ì¦ˆ ì‹œë„
    try:
        from PIL import Image

        img = Image.open(local)
        # ìµœëŒ€ ê°€ë¡œ/ì„¸ë¡œ 1024pxë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        resample = getattr(Image, "Resampling", Image).LANCZOS
        img.thumbnail((1024, 1024), resample)
        ext = os.path.splitext(local)[1].lower()
        quality = 75  # ìˆ˜ì •: í’ˆì§ˆ ë‚®ì¶° ìš©ëŸ‰ ì¤„ì´ê¸°
        if ext in (".jpg", ".jpeg"):
            img.save(local, format="JPEG", optimize=True, quality=quality)
        else:
            # PNG ë“±ì€ JPEGë¡œ ë³€í™˜
            rgb = img.convert("RGB")
            jpeg_path = os.path.splitext(local)[0] + ".jpg"
            rgb.save(jpeg_path, format="JPEG", optimize=True, quality=quality)
            local = jpeg_path
    except ImportError:
        # PIL ë¯¸ì„¤ì¹˜ ì‹œ ì••ì¶• ìƒëµ
        pass
    except Exception as e:
        print(f"[ì••ì¶• ì˜¤ë¥˜] {local}: {e}")
    # S3 ì—…ë¡œë“œ
    import mimetypes

    ctype, _ = mimetypes.guess_type(local)
    s3.upload_file(
        Filename=local,
        Bucket=S3_BUCKET,
        Key=key,
        ExtraArgs={
            "ACL": "public-read",
            "ContentType": ctype or "application/octet-stream",
        },
    )
    return f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{key}"


def get_smore_cookies():
    driver_path = chromedriver_autoinstaller.install()
    opts = Options()
    opts.binary_location = CHROME_BINARY

    # GitHub Actions í™˜ê²½ìš© ì˜µì…˜ ì¶”ê°€
    opts.add_argument("--headless")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--remote-debugging-port=9222")

    # ê¸°ì¡´ ì˜µì…˜
    if SMORE_USER_DATA:
        opts.add_argument(f"--user-data-dir={SMORE_USER_DATA}")
    if SMORE_PROFILE:
        opts.add_argument(f"--profile-directory={SMORE_PROFILE}")

    driver = webdriver.Chrome(service=Service(driver_path), options=opts)
    try:
        driver.get("https://smore.im")
        return {c["name"]: c["value"] for c in driver.get_cookies()}
    finally:
        driver.quit()


def download_smore_image(page_url, cookies, row_idx):
    driver_path = chromedriver_autoinstaller.install()
    opts = Options()
    opts.binary_location = CHROME_BINARY
    opts.add_argument(f"--user-data-dir={SMORE_USER_DATA}")
    opts.add_argument(f"--profile-directory={SMORE_PROFILE}")
    driver = webdriver.Chrome(service=Service(driver_path), options=opts)
    try:
        driver.get(page_url)
        link = WebDriverWait(driver, 15).until(
            EC.element_to_be_clickable((By.ID, "download"))
        )
        file_url = link.get_attribute("href")
        sess = requests.Session()
        sess.cookies.update(cookies)
        resp = sess.get(file_url, stream=True, timeout=30)
    finally:
        driver.quit()

    resp.raise_for_status()
    if "text/html" in resp.headers.get("Content-Type", ""):
        raise RuntimeError("HTML ë°˜í™˜ë¨(ë¡œê·¸ì¸ í•„ìš”?): " + page_url)

    cd = resp.headers.get("content-disposition", "")
    m = re.search(r"filename\*=UTF-8''([^;]+)", cd) or re.search(
        r"filename=?\"?([^\";]+)\"?", cd
    )
    orig = unquote(m.group(1)) if m else os.path.basename(urlparse(file_url).path)
    filename = safe_key_name(row_idx, orig)
    local = os.path.join(DOWNLOAD_DIR, filename)
    with open(local, "wb") as f:
        for chunk in resp.iter_content(1024):
            f.write(chunk)
    return local


# â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sheet_to_notion_s3():
    cookies = get_smore_cookies()
    res = (
        sheets.spreadsheets()
        .values()
        .get(
            spreadsheetId=SPREADSHEET_ID, range=SHEET_RANGE, valueRenderOption="FORMULA"
        )
        .execute()
    )
    rows = res.get("values", [])
    if len(rows) < 2:
        print("ë°ì´í„° ì—†ìŒ")
        return

    headers = [normalize_header(h) for h in rows[0]]
    done_idx = headers.index("ì´ê´€ì™„ë£Œ") if "ì´ê´€ì™„ë£Œ" in headers else None
    data_rows = rows[1:]
    recs = data_rows[TEST_OFFSET : TEST_OFFSET + TEST_LIMIT]
    db_props = notion.databases.retrieve(database_id=NOTION_DB_ID)["properties"]

    for i, row in enumerate(recs, start=TEST_OFFSET + 2):
        # â”€â”€â”€ ì—¬ê¸°ì„œ Sì—´ 'ì™„ë£Œ' ì²´í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if done_idx is not None and len(row) > done_idx:
            cell_val = row[done_idx].strip()
            # 'ì™„ë£Œ'ë¡œ ì‹œì‘(ì˜ˆ: "ì™„ë£Œ", "ì™„ë£Œ<")í•˜ë©´ ê±´ë„ˆëœë‹ˆë‹¤
            if cell_val.startswith("ì™„ë£Œ"):
                print(f"[Row {i}] ì´ë¯¸ ì™„ë£Œëœ í–‰, ê±´ë„ˆëœë‹ˆë‹¤")
                continue
        data = {headers[j]: row[j] if j < len(row) else "" for j in range(len(headers))}
        props, image_urls = {}, []

        for hdr, val in data.items():
            if not val:
                continue
            prop = NORM_HEADER_TO_PROP.get(normalize_header(hdr))
            if not prop or prop not in db_props:
                continue
            ptype = db_props[prop]["type"]
            if ptype == "files":
                # m = re.match(r'=HYPERLINK\("([^"\)]+)"', val)
                # url = m.group(1) if m else val
                m = re.match(r'=HYPERLINK\("([^"]+)"(?:\s*,\s*"([^"]+)")?\)', val)
                url = m.group(1)  # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL
                disp = m.group(2) if m and m.group(2) else None  # ì˜µì…˜ í‘œì‹œëª…
                try:
                    local = download_smore_image(url, cookies, i)
                    key = safe_key_name(i, os.path.basename(local))
                    s3_url = upload_to_s3(local, key)
                    original_name = disp or os.path.basename(local)
                    if len(original_name) > 100:
                        display_name = original_name[:97] + "..."
                    else:
                        display_name = original_name
                    props[prop] = {
                        "files": [
                            {
                                # "name": os.path.basename(local),
                                "name": display_name,
                                "external": {"url": s3_url},
                            }
                        ]
                    }
                    image_urls.append(s3_url)
                except Exception as e:
                    print(f"[Row {i}] íŒŒì¼ ì˜¤ë¥˜: {e}")
                continue
            if ptype == "date":
                m2 = re.match(
                    r"^=DATE\(\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*\)", str(val)
                )
                if m2:
                    y, mo, da = map(int, m2.groups())
                    props[prop] = {"date": {"start": datetime(y, mo, da).isoformat()}}
                else:
                    dt = excel_serial_to_datetime(val)
                    if dt:
                        props[prop] = {"date": {"start": dt.isoformat()}}
                continue
            if ptype == "url":
                props[prop] = {"url": val.strip()}
                continue
            if ptype == "title":
                props[prop] = {"title": [{"text": {"content": str(val)}}]}
                continue
            if ptype == "rich_text":
                props[prop] = {"rich_text": [{"text": {"content": str(val)}}]}
                continue

        if props:
            # print(f"[Row {i}] props=", props)  # debug
            page = notion.pages.create(
                parent={"database_id": NOTION_DB_ID}, properties=props
            )
            print(f"[Row {i}] Notion ë“±ë¡ ì™„ë£Œ", list(props.keys()))

            # ìˆ˜ì •: Google Sheets Sì—´ì— 'ì™„ë£Œ' í‘œì‹œ
            cell = f"{SHEET_NAME}!S{i}"
            sheets.spreadsheets().values().update(
                spreadsheetId=SPREADSHEET_ID,
                range=cell,
                valueInputOption="USER_ENTERED",
                body={"values": [["ì™„ë£Œ"]]},
            ).execute()
            print(f"[Row {i}] ì‹œíŠ¸ì— â€˜ì™„ë£Œâ€™ í‘œê¸° ì™„ë£Œ â†’ {cell}")

            # â”€â”€â”€ ë¸”ë¡ ì‚½ì… (ì˜¤ë¥˜ ë‚˜ë©´ ê±´ë„ˆë›°ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            for url in image_urls:
                ext = os.path.splitext(urlparse(url).path)[1].lower()
                if ext == ".pdf":
                    block = {
                        "object": "block",
                        "type": "file",
                        "file": {"type": "external", "external": {"url": url}},
                    }
                else:
                    block = {
                        "object": "block",
                        "type": "image",
                        "image": {"type": "external", "external": {"url": url}},
                    }

                try:
                    notion.blocks.children.append(block_id=page["id"], children=[block])
                    print(f"[Row {i}] ë¸”ë¡ ì‚½ì… ì™„ë£Œ â†’ {url}")
                except Exception as e:
                    print(f"[Row {i}] ë¸”ë¡ ì‚½ì… ì˜¤ë¥˜, ê±´ë„ˆëœë‹ˆë‹¤: {url} â–¶ {e}")

        else:
            print(f"[Row {i}] ë§¤í•‘ëœ ì†ì„± ì—†ìŒ")


if __name__ == "__main__":
    sheet_to_notion_s3()
